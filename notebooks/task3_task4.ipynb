{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqDrO1YISuQ5",
        "outputId": "9a5f968f-f65c-411b-aaad-44865a63a3de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=d57b4335ed7e0c28679a12e2f343a9bebed72589c28983d501603c1aeabe3104\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
            "Successfully built seqeval\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, seqeval, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "15Gi6wPvSltd"
      },
      "outputs": [],
      "source": [
        "import transformers, datasets, torch, seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/processed\n",
        "!mkdir -p results/fine_tuned_ner_model"
      ],
      "metadata": {
        "id": "ZrkF89X-Tkqh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('fine_tune_ner.py', 'w') as f:\n",
        "    f.write('''\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['WANDB_MODE'] = 'disabled'\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from seqeval.metrics import classification_report\n",
        "import numpy as np\n",
        "import torch\n",
        "import logging\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Train NER model')\n",
        "parser.add_argument('--model_name', type=str, required=True, help='Model name')\n",
        "parser.add_argument('--output_dir', type=str, required=True, help='Output directory')\n",
        "args = parser.parse_args()\n",
        "\n",
        "def load_conll(file_path):\n",
        "    sentences, labels = [], []\n",
        "    current_sentence, current_labels = [], []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    token, label = line.strip().split()\n",
        "                    current_sentence.append(token)\n",
        "                    current_labels.append(label)\n",
        "                else:\n",
        "                    if current_sentence:\n",
        "                        sentences.append(current_sentence)\n",
        "                        labels.append(current_labels)\n",
        "                        current_sentence, current_labels = [], []\n",
        "            if current_sentence:\n",
        "                sentences.append(current_sentence)\n",
        "                labels.append(current_labels)\n",
        "        return Dataset.from_dict({'tokens': sentences, 'ner_tags': labels})\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading CoNLL: {e}\")\n",
        "        raise\n",
        "\n",
        "logging.info(\"Loading CoNLL dataset\")\n",
        "dataset = load_conll('/content/labeled_data.conll')\n",
        "\n",
        "label_list = sorted(set(label for sent in dataset['ner_tags'] for label in sent))\n",
        "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "def convert_labels_to_ids(example):\n",
        "    example['ner_tags'] = [label2id[label] for label in example['ner_tags']]\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(convert_labels_to_ids)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_val_test = dataset.train_test_split(train_size=train_size, test_size=val_size+test_size, seed=42)\n",
        "val_test = train_val_test['test'].train_test_split(train_size=val_size/(val_size+test_size), seed=42)\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_val_test['train'],\n",
        "    'validation': val_test['train'],\n",
        "    'test': val_test['test']\n",
        "})\n",
        "\n",
        "logging.info(f\"Loading tokenizer for {args.model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True, padding=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples['ner_tags']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        aligned_labels = [-100 if word_id is None else label[word_id] for word_id in word_ids]\n",
        "        labels.append(aligned_labels)\n",
        "    tokenized_inputs['labels'] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "logging.info(\"Tokenizing dataset\")\n",
        "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "logging.info(f\"Saving tokenized dataset for {args.model_name}\")\n",
        "try:\n",
        "    os.makedirs(f'/content/data/processed/tokenized_dataset_{args.model_name.split(\"/\")[-1]}', exist_ok=True)\n",
        "    tokenized_dataset.save_to_disk(f'/content/data/processed/tokenized_dataset_{args.model_name.split(\"/\")[-1]}')\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error saving tokenized dataset: {e}\")\n",
        "    raise\n",
        "\n",
        "logging.info(f\"Loading model {args.model_name}\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    args.model_name,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
        "    pred_labels = [[id2label[p] for p, l in zip(pred, label) if l != -100] for pred, label in zip(predictions, labels)]\n",
        "    results = classification_report(true_labels, pred_labels, output_dict=True, zero_division=0)\n",
        "    return {\n",
        "        'precision': results['weighted avg']['precision'],\n",
        "        'recall': results['weighted avg']['recall'],\n",
        "        'f1': results['weighted avg']['f1-score']\n",
        "    }\n",
        "\n",
        "def measure_inference_time(model, tokenizer, text=\"Adidas SAMBAROSE ዋጋ 3300 ብር መገናኛ\"):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to('cuda')\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs).logits\n",
        "    return time.time() - start_time\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=args.output_dir,\n",
        "    eval_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='/content/logs',\n",
        "    logging_steps=10,\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1',\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['validation'],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "logging.info(f\"Starting training for {args.model_name}\")\n",
        "trainer.train()\n",
        "\n",
        "logging.info(f\"Evaluating {args.model_name}\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation Results for {args.model_name}:\")\n",
        "print(eval_results)\n",
        "\n",
        "inference_time = measure_inference_time(model, tokenizer)\n",
        "print(f\"Inference Time for {args.model_name}: {inference_time:.4f} seconds\")\n",
        "\n",
        "logging.info(f\"Saving model to {args.output_dir}\")\n",
        "trainer.save_model(args.output_dir)\n",
        "\n",
        "logging.info(f\"Generating test set predictions for {args.model_name}\")\n",
        "predictions = trainer.predict(tokenized_dataset['test'])\n",
        "print(f\"Test Set Predictions for {args.model_name}:\")\n",
        "print(predictions.metrics)\n",
        "    ''')"
      ],
      "metadata": {
        "id": "IJgBUMWuToFM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/results/fine_tuned_ner_model\n",
        "!python fine_tune_ner.py --model_name xlm-roberta-base --output_dir /content/results/fine_tuned_ner_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zVVKip_TxyK",
        "outputId": "d518a5ab-ae3e-4b01-a566-3855bb33c374"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750797558.318748   25418 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750797558.364455   25418 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Map: 100% 50/50 [00:00<00:00, 4224.81 examples/s]\n",
            "Map: 100% 40/40 [00:00<00:00, 776.64 examples/s]\n",
            "Map: 100% 5/5 [00:00<00:00, 466.08 examples/s]\n",
            "Map: 100% 5/5 [00:00<00:00, 511.01 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 40/40 [00:00<00:00, 8770.11 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 5/5 [00:00<00:00, 1210.97 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 5/5 [00:00<00:00, 1301.61 examples/s]\n",
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            " 20% 5/25 [03:40<13:30, 40.53s/it]\n",
            "                                  \n",
            "\u001b[A{'eval_loss': 1.0820186138153076, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 3.5877, 'eval_samples_per_second': 1.394, 'eval_steps_per_second': 0.279, 'epoch': 1.0}\n",
            " 20% 5/25 [03:44<13:30, 40.53s/it]\n",
            "100% 1/1 [00:00<00:00, 78.90it/s]\u001b[A\n",
            "{'loss': 1.156, 'grad_norm': 8.206328392028809, 'learning_rate': 1.2800000000000001e-05, 'epoch': 2.0}\n",
            " 40% 10/25 [07:30<10:15, 41.01s/it]\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.2423413246870041, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 3.5365, 'eval_samples_per_second': 1.414, 'eval_steps_per_second': 0.283, 'epoch': 2.0}\n",
            " 40% 10/25 [07:33<10:15, 41.01s/it]\n",
            "100% 1/1 [00:00<00:00, 106.77it/s]\u001b[A\n",
            " 60% 15/25 [12:01<07:31, 45.18s/it]\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.22282585501670837, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 4.9828, 'eval_samples_per_second': 1.003, 'eval_steps_per_second': 0.201, 'epoch': 3.0}\n",
            " 60% 15/25 [12:06<07:31, 45.18s/it]\n",
            "100% 1/1 [00:00<00:00, 120.13it/s]\u001b[A\n",
            "{'loss': 0.4331, 'grad_norm': 1.487792730331421, 'learning_rate': 4.800000000000001e-06, 'epoch': 4.0}\n",
            " 80% 20/25 [16:16<03:38, 43.78s/it]\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.17261427640914917, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 3.6027, 'eval_samples_per_second': 1.388, 'eval_steps_per_second': 0.278, 'epoch': 4.0}\n",
            " 80% 20/25 [16:20<03:38, 43.78s/it]\n",
            "100% 1/1 [00:00<00:00, 121.82it/s]\u001b[A\n",
            "100% 25/25 [20:36<00:00, 43.67s/it]\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.1593489944934845, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 3.5107, 'eval_samples_per_second': 1.424, 'eval_steps_per_second': 0.285, 'epoch': 5.0}\n",
            "100% 25/25 [20:39<00:00, 43.67s/it]\n",
            "100% 1/1 [00:00<00:00, 118.96it/s]\u001b[A\n",
            "{'train_runtime': 1327.4826, 'train_samples_per_second': 0.151, 'train_steps_per_second': 0.019, 'train_loss': 0.7133010768890381, 'epoch': 5.0}\n",
            "100% 25/25 [22:07<00:00, 53.10s/it]\n",
            "100% 1/1 [00:00<00:00, 108.46it/s]\n",
            "Evaluation Results for xlm-roberta-base:\n",
            "{'eval_loss': 1.0820186138153076, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 3.928, 'eval_samples_per_second': 1.273, 'eval_steps_per_second': 0.255, 'epoch': 5.0}\n",
            "Inference Time for xlm-roberta-base: 0.1196 seconds\n",
            "100% 1/1 [00:00<00:00, 136.66it/s]\n",
            "Test Set Predictions for xlm-roberta-base:\n",
            "{'test_loss': 1.0451884269714355, 'test_precision': 0.0, 'test_recall': 0.0, 'test_f1': 0.0, 'test_runtime': 4.6154, 'test_samples_per_second': 1.083, 'test_steps_per_second': 0.217}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/results/fine_tuned_mbert\n",
        "!python fine_tune_ner.py --model_name bert-base-multilingual-cased --output_dir /content/results/fine_tuned_mbert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXRAPPLoT4Xv",
        "outputId": "9aad471a-0e21-4df6-8e53-3ce6da4f5569"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750799034.141852   31292 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750799034.153644   31292 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Map: 100% 50/50 [00:00<00:00, 4010.54 examples/s]\n",
            "Map: 100% 40/40 [00:00<00:00, 828.07 examples/s]\n",
            "Map: 100% 5/5 [00:00<00:00, 496.18 examples/s]\n",
            "Map: 100% 5/5 [00:00<00:00, 553.54 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 40/40 [00:00<00:00, 7803.72 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 5/5 [00:00<00:00, 1261.29 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 5/5 [00:00<00:00, 1239.74 examples/s]\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            " 20% 5/25 [02:12<08:32, 25.64s/it]\n",
            "                                  \n",
            "\u001b[A{'eval_loss': 0.12002787739038467, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 2.8833, 'eval_samples_per_second': 1.734, 'eval_steps_per_second': 0.347, 'epoch': 1.0}\n",
            " 20% 5/25 [02:15<08:32, 25.64s/it]\n",
            "100% 1/1 [00:00<00:00, 130.22it/s]\u001b[A\n",
            "{'loss': 0.5046, 'grad_norm': 0.6825437545776367, 'learning_rate': 1.2800000000000001e-05, 'epoch': 2.0}\n",
            " 40% 10/25 [05:12<07:14, 29.00s/it]\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.09100799262523651, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 3.1039, 'eval_samples_per_second': 1.611, 'eval_steps_per_second': 0.322, 'epoch': 2.0}\n",
            " 40% 10/25 [05:15<07:14, 29.00s/it]\n",
            "100% 1/1 [00:00<00:00, 141.81it/s]\u001b[A\n",
            " 60% 15/25 [08:11<04:54, 29.46s/it]\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.0727657899260521, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 3.837, 'eval_samples_per_second': 1.303, 'eval_steps_per_second': 0.261, 'epoch': 3.0}\n",
            " 60% 15/25 [08:15<04:54, 29.46s/it]\n",
            "100% 1/1 [00:00<00:00, 124.83it/s]\u001b[A\n",
            "{'loss': 0.2287, 'grad_norm': 0.9656631946563721, 'learning_rate': 4.800000000000001e-06, 'epoch': 4.0}\n",
            " 80% 20/25 [11:20<02:30, 30.11s/it]\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.07743837684392929, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 2.9882, 'eval_samples_per_second': 1.673, 'eval_steps_per_second': 0.335, 'epoch': 4.0}\n",
            " 80% 20/25 [11:23<02:30, 30.11s/it]\n",
            "100% 1/1 [00:00<00:00, 143.95it/s]\u001b[A\n",
            "100% 25/25 [14:24<00:00, 29.75s/it]\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.07116679847240448, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 3.6799, 'eval_samples_per_second': 1.359, 'eval_steps_per_second': 0.272, 'epoch': 5.0}\n",
            "100% 25/25 [14:28<00:00, 29.75s/it]\n",
            "100% 1/1 [00:00<00:00, 142.31it/s]\u001b[A\n",
            "{'train_runtime': 924.7482, 'train_samples_per_second': 0.216, 'train_steps_per_second': 0.027, 'train_loss': 0.3354925537109375, 'epoch': 5.0}\n",
            "100% 25/25 [15:24<00:00, 36.99s/it]\n",
            "100% 1/1 [00:00<00:00, 125.87it/s]\n",
            "Evaluation Results for bert-base-multilingual-cased:\n",
            "{'eval_loss': 0.12002787739038467, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 3.6543, 'eval_samples_per_second': 1.368, 'eval_steps_per_second': 0.274, 'epoch': 5.0}\n",
            "Inference Time for bert-base-multilingual-cased: 0.0993 seconds\n",
            "100% 1/1 [00:00<00:00, 201.24it/s]\n",
            "Test Set Predictions for bert-base-multilingual-cased:\n",
            "{'test_loss': 0.16170494258403778, 'test_precision': 0.0, 'test_recall': 0.0, 'test_f1': 0.0, 'test_runtime': 2.3938, 'test_samples_per_second': 2.089, 'test_steps_per_second': 0.418}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/results/fine_tuned_distilbert\n",
        "!python fine_tune_ner.py --model_name distilbert-base-multilingual-cased --output_dir /content/results/fine_tuned_distilbert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-Vw2g8tUBJQ",
        "outputId": "657c4121-5307-4aca-a966-22857d3b8302"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750800081.474612   35466 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750800081.493098   35466 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Map: 100% 50/50 [00:00<00:00, 4036.87 examples/s]\n",
            "Map: 100% 40/40 [00:00<00:00, 796.50 examples/s]\n",
            "Map: 100% 5/5 [00:00<00:00, 531.61 examples/s]\n",
            "Map: 100% 5/5 [00:00<00:00, 533.52 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 40/40 [00:00<00:00, 6931.59 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 5/5 [00:00<00:00, 1013.21 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 5/5 [00:00<00:00, 1030.04 examples/s]\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            " 20% 5/25 [01:21<05:10, 15.54s/it]\n",
            "                                  \n",
            "\u001b[A{'eval_loss': 0.7808218002319336, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 1.4346, 'eval_samples_per_second': 3.485, 'eval_steps_per_second': 0.697, 'epoch': 1.0}\n",
            " 20% 5/25 [01:22<05:10, 15.54s/it]\n",
            "100% 1/1 [00:00<00:00, 120.84it/s]\u001b[A\n",
            "{'loss': 0.9719, 'grad_norm': 2.9171628952026367, 'learning_rate': 1.2800000000000001e-05, 'epoch': 2.0}\n",
            " 40% 10/25 [02:46<03:45, 15.06s/it]\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.3053916096687317, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 2.1712, 'eval_samples_per_second': 2.303, 'eval_steps_per_second': 0.461, 'epoch': 2.0}\n",
            " 40% 10/25 [02:48<03:45, 15.06s/it]\n",
            "100% 1/1 [00:00<00:00, 142.36it/s]\u001b[A\n",
            " 60% 15/25 [04:10<02:28, 14.85s/it]\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.14519274234771729, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 1.4041, 'eval_samples_per_second': 3.561, 'eval_steps_per_second': 0.712, 'epoch': 3.0}\n",
            " 60% 15/25 [04:12<02:28, 14.85s/it]\n",
            "100% 1/1 [00:00<00:00, 143.54it/s]\u001b[A\n",
            "{'loss': 0.3034, 'grad_norm': 0.7772067189216614, 'learning_rate': 4.800000000000001e-06, 'epoch': 4.0}\n",
            " 80% 20/25 [05:49<01:19, 15.83s/it]\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.11024029552936554, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 1.4198, 'eval_samples_per_second': 3.522, 'eval_steps_per_second': 0.704, 'epoch': 4.0}\n",
            " 80% 20/25 [05:50<01:19, 15.83s/it]\n",
            "100% 1/1 [00:00<00:00, 136.05it/s]\u001b[A\n",
            "100% 25/25 [07:21<00:00, 15.53s/it]\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.10516442358493805, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 2.3197, 'eval_samples_per_second': 2.155, 'eval_steps_per_second': 0.431, 'epoch': 5.0}\n",
            "100% 25/25 [07:24<00:00, 15.53s/it]\n",
            "100% 1/1 [00:00<00:00, 87.36it/s]\u001b[A\n",
            "{'train_runtime': 473.1592, 'train_samples_per_second': 0.423, 'train_steps_per_second': 0.053, 'train_loss': 0.5634280824661255, 'epoch': 5.0}\n",
            "100% 25/25 [07:53<00:00, 18.93s/it]\n",
            "100% 1/1 [00:00<00:00, 121.22it/s]\n",
            "Evaluation Results for distilbert-base-multilingual-cased:\n",
            "{'eval_loss': 0.7808218002319336, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 1.3931, 'eval_samples_per_second': 3.589, 'eval_steps_per_second': 0.718, 'epoch': 5.0}\n",
            "Inference Time for distilbert-base-multilingual-cased: 0.0508 seconds\n",
            "100% 1/1 [00:00<00:00, 115.95it/s]\n",
            "Test Set Predictions for distilbert-base-multilingual-cased:\n",
            "{'test_loss': 0.7196463346481323, 'test_precision': 0.0, 'test_recall': 0.0, 'test_f1': 0.0, 'test_runtime': 1.4207, 'test_samples_per_second': 3.519, 'test_steps_per_second': 0.704}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('compare_models.py', 'w') as f:\n",
        "    f.write('''import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['WANDB_MODE'] = 'disabled'\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from datasets import load_from_disk\n",
        "from seqeval.metrics import classification_report\n",
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import logging\n",
        "import glob\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def measure_inference_time(model, tokenizer, text=\"Adidas SAMBAROSE ዋጋ 3300 ብር መገናኛ\", num_runs=100):\n",
        "model.eval()\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to('cuda')\n",
        "total_time = 0\n",
        "for _ in range(num_runs):\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "outputs = model(**inputs).logits\n",
        "total_time += time.time() - start_time\n",
        "return total_time / num_runs\n",
        "\n",
        "def evaluate_model(model, tokenizer, dataset, id2label):\n",
        "model.eval()\n",
        "true_labels, pred_labels = [], []\n",
        "for example in dataset:\n",
        "inputs = tokenizer(example['tokens'], is_split_into_words=True, return_tensors=\"pt\", truncation=True, padding=True).to('cuda')\n",
        "with torch.no_grad():\n",
        "outputs = model(**inputs).logits\n",
        "predictions = torch.argmax(outputs, dim=2)[0]\n",
        "labels = example['labels']\n",
        "word_ids = inputs.word_ids()\n",
        "example_true, example_pred = [], []\n",
        "for i, (pred, label) in enumerate(zip(predictions, labels)):\n",
        "if word_ids[i] is not None and label != -100:\n",
        "example_true.append(id2label[label])\n",
        "example_pred.append(id2label[pred.item()])\n",
        "true_labels.append(example_true)\n",
        "pred_labels.append(example_pred)\n",
        "results = classification_report(true_labels, pred_labels, output_dict=True, zero_division=0)\n",
        "return {\n",
        "'precision': results['weighted avg']['precision'],\n",
        "'recall': results['weighted avg']['recall'],\n",
        "'f1': results['weighted avg']['f1-score']\n",
        "}\n",
        "\n",
        "def get_model_size(model_dir):\n",
        "total_size = 0\n",
        "for file in glob.glob(f\"{model_dir}/*\"):\n",
        "total_size += os.path.getsize(file)\n",
        "return total_size / (1024 ** 2)\n",
        "\n",
        "logging.info(\"Loading tokenized dataset\")\n",
        "tokenized_dataset = load_from_disk('file:///content/data/processed/tokenized_dataset_xlm-roberta-base')\n",
        "\n",
        "models = [\n",
        "{'name': 'XLM-RoBERTa', 'path': '/results/fine_tuned_ner_model', 'model_name': 'xlm-roberta-base'},\n",
        "{'name': 'mBERT', 'path': '/results/fine_tuned_mbert', 'model_name': 'bert-base-multilingual-cased'},\n",
        "{'name': 'DistilBERT', 'path': '/results/fine_tuned_distilbert', 'model_name': 'distilbert-base-multilingual-cased'}\n",
        "]\n",
        "\n",
        "comparison = []\n",
        "for config in models:\n",
        "logging.info(f\"Evaluating {config['name']}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "model = AutoModelForTokenClassification.from_pretrained(config['path']).to('cuda')\n",
        "eval_results = evaluate_model(model, tokenizer, tokenized_dataset['validation'], model.config.id2label)\n",
        "inference_time = measure_inference_time(model, tokenizer)\n",
        "model_size = get_model_size(config['path'])\n",
        "comparison.append({\n",
        "'Model': config['name'],\n",
        "'F1-Score': eval_results['f1'],\n",
        "'Precision': eval_results['precision'],\n",
        "'Recall': eval_results['recall'],\n",
        "'Inference Time (s)': inference_time,\n",
        "'Model Size (MB)': model_size\n",
        "})\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison)\n",
        "print(\"Model Comparison Table:\")\n",
        "print(comparison_df)\n",
        "comparison_df.to_csv('/results/model_comparison.csv', index=False)\n",
        "logging.info(\"Comparison table saved to /results/model_comparison.csv\")''')"
      ],
      "metadata": {
        "id": "sz_c1eejUHRl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python compare_models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnzTxSi-UKU5",
        "outputId": "f3f530e7-7353-4bb6-f293-83018e855293"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-24 21:30:44,277 - INFO - Loading tokenized dataset\n",
            "2025-06-24 21:30:44,305 - INFO - Evaluating XLM-RoBERTa\n",
            "2025-06-24 21:30:46,597 - INFO - Fallback to latest checkpoint: /content/results/fine_tuned_ner_model/checkpoint-25\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750800650.515069   37743 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750800650.526954   37743 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-24 21:31:23,429 - INFO - Evaluating mBERT\n",
            "2025-06-24 21:31:24,538 - INFO - Fallback to latest checkpoint: /content/results/fine_tuned_mbert/checkpoint-25\n",
            "2025-06-24 21:31:41,118 - INFO - Evaluating DistilBERT\n",
            "2025-06-24 21:31:41,803 - INFO - Fallback to latest checkpoint: /content/results/fine_tuned_distilbert/checkpoint-25\n",
            "Model Comparison Table:\n",
            "         Model  F1-Score  ...  Inference Time (s)  Model Size (MB)\n",
            "0  XLM-RoBERTa       0.0  ...            0.173242      3175.450049\n",
            "1        mBERT       0.0  ...            0.109796      2028.863564\n",
            "2   DistilBERT       0.0  ...            0.050041      1542.075182\n",
            "\n",
            "[3 rows x 6 columns]\n",
            "2025-06-24 21:31:49,923 - INFO - Comparison table saved to /content/results/model_comparison.csv\n"
          ]
        }
      ]
    }
  ]
}